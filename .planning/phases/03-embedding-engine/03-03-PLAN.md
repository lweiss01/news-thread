---
wave: 2
depends_on: [03-01]
files_modified:
  - app/src/main/java/com/example/newsthread/embedding/EmbeddingModelManager.kt (new)
  - app/src/main/java/com/example/newsthread/embedding/BertTokenizerWrapper.kt (new)
  - app/src/main/java/com/example/newsthread/di/EmbeddingModule.kt (new)
autonomous: false
---

# Plan 03-03: Embedding Generation Engine

## Objective

Implement the core TensorFlow Lite embedding generation engine with model loading, tokenization, and inference logic.

## Context

From 03-RESEARCH.md:
- Use TF Lite Interpreter API with BertTokenizer
- Load model from assets as MappedByteBuffer  
- Run inference on Dispatchers.Default (never main thread)
- Handle text truncation at sentence boundary

From 03-CONTEXT.md:
- Primary text: first ~1000 chars of extracted article text
- Fallback: title + description
- Dynamic text length based on device RAM (<2GB: 2500 chars, >4GB: 5000 chars)

## Tasks

### Task 1: Create Embedding Model Manager
<task>
<action>
Create `EmbeddingModelManager.kt` as a singleton:

```kotlin
import android.content.Context
import android.content.res.AssetFileDescriptor
import android.content.res.AssetManager
import org.tensorflow.lite.Interpreter
import org.tensorflow.lite.support.common.FileUtil
import timber.log.Timber
import java.io.FileInputStream
import java.nio.MappedByteBuffer
import java.nio.channels.FileChannel
import javax.inject.Inject
import javax.inject.Singleton

@Singleton
class EmbeddingModelManager @Inject constructor(
    private val context: Context
) {
    private var interpreter: Interpreter? = null
    private var tokenizer: BertTokenizerWrapper? = null
    
    @Synchronized
    fun initialize() {
        if (interpreter != null) {
            Timber.d("EmbeddingModelManager already initialized")
            return
        }
        
        try {
            val modelBuffer = loadModelFile(context.assets, "sentence_model_v1.tflite")
            val options = Interpreter.Options().apply {
                setNumThreads(4)  // Mid-range devices: 4 threads
            }
            interpreter = Interpreter(modelBuffer, options)
            tokenizer = BertTokenizerWrapper(context, "vocab.txt")
            Timber.i("TF Lite model initialized successfully")
        } catch (e: Exception) {
            Timber.e(e, "Failed to initialize TF Lite model")
            throw EmbeddingInitializationException("Model initialization failed", e)
        }
    }
    
    private fun loadModelFile(assets: AssetManager, filename: String): MappedByteBuffer {
        val fileDescriptor: AssetFileDescriptor = assets.openFd(filename)
        val inputStream = FileInputStream(fileDescriptor.fileDescriptor)
        val fileChannel = inputStream.channel
        val startOffset = fileDescriptor.startOffset
        val declaredLength = fileDescriptor.declaredLength
        return fileChannel.map(FileChannel.MapMode.READ_ONLY, startOffset, declaredLength)
    }
    
    fun generateEmbedding(text: String): FloatArray? {
        val currentInterpreter = interpreter
        val currentTokenizer = tokenizer
        
        if (currentInterpreter == null || currentTokenizer == null) {
            Timber.w("Model not initialized, cannot generate embedding")
            return null
        }
        
        return try {
            // Tokenize with truncation
            val truncatedText = truncateAtSentenceBoundary(text, getMaxTextLength())
            val tokens = currentTokenizer.tokenize(truncatedText)
            
            // Convert to input tensors
            val inputIds = currentTokenizer.convertTokensToIds(tokens)
            val attentionMask = IntArray(inputIds.size) { 1 }
            
            // Pad to 256 (model max length)
            val paddedInputIds = padSequence(input Ids, 256)
            val paddedAttentionMask = padSequence(attentionMask, 256)
            
            // Prepare input/output tensors
            val inputIdsTensor = Array(1) { paddedInputIds }
            val attentionMaskTensor = Array(1) { paddedAttentionMask }
            val outputEmbedding = Array(1) { FloatArray(384) }
            
            // Run inference
            val inputs = arrayOf(inputIdsTensor, attentionMaskTensor)
            val outputs = mapOf(0 to outputEmbedding)
            currentInterpreter.runForMultipleInputsOutputs(inputs, outputs)
            
            outputEmbedding[0]  // Return 384-dim embedding
            
        } catch (e: Exception) {
            Timber.e(e, "Embedding generation failed for text: ${text.take(50)}...")
            null
        }
    }
    
    private fun truncateAtSentenceBoundary(text: String, maxLength: Int): String {
        if (text.length <= maxLength) return text
        
        // Find last sentence boundary before maxLength
        val truncated = text.substring(0, maxLength)
        val lastPeriod = truncated.lastIndexOf('.')
        val lastQuestion = truncated.lastIndexOf('?')
        val lastExclamation = truncated.lastIndexOf('!')
        
        val sentenceBoundary = maxOf(lastPeriod, lastQuestion, lastExclamation)
        return if (sentenceBoundary > 0) {
            truncated.substring(0, sentenceBoundary + 1)
        } else {
            truncated  // No sentence boundary found, hard truncate
        }
    }
    
    private fun getMaxTextLength(): Int {
        val activityManager = context.getSystemService(Context.ACTIVITY_SERVICE) as ActivityManager
        val memoryInfo = ActivityManager.MemoryInfo()
        activityManager.getMemoryInfo(memoryInfo)
        val totalRamGB = memoryInfo.totalMem / (1024 * 1024 * 1024)
        
        return when {
            totalRamGB < 2 -> 2500  // Low-end devices
            totalRamGB > 4 -> 5000  // High-end devices
            else -> 3500            // Mid-range
        }
    }
    
    private fun padSequence(sequence: IntArray, targetLength: Int): IntArray {
        return sequence.copyOf(targetLength).apply {
            if (sequence.size < targetLength) {
                fill(0, sequence.size, targetLength)  // Pad with 0
            }
        }
    }
    
    fun close() {
        interpreter?.close()
        interpreter = null
        tokenizer = null
        Timber.d("TF Lite model resources released")
    }
}

class EmbeddingInitializationException(message: String, cause: Throwable?) : Exception(message, cause)
```
</action>
<test>
Code compiles. EmbeddingModelManager can be injected via Hilt.
</test>
</task>

### Task 2: Create BertTokenizer Wrapper
<task>
<action>
Create `BertTokenizerWrapper.kt`:

```kotlin
import android.content.Context
import org.tensorflow.lite.support.label.ops.LabelFileTokenizer
import java.io.BufferedReader
import java.io.InputStreamReader

class BertTokenizerWrapper(
    context: Context,
    vocabFilename: String
) {
    private val vocab: Map<String, Int>
    private val idsToTokens: Map<Int, String>
    
    init {
        val vocabList = context.assets.open(vocabFilename).use { inputStream ->
            BufferedReader(InputStreamReader(inputStream)).readLines()
        }
        vocab = vocabList.mapIndexed { index, token -> token to index }.toMap()
        idsToTokens = vocabList.mapIndexed { index, token -> index to token }.toMap()
    }
    
    fun tokenize(text: String): List<String> {
        val tokens = mutableListOf<String>()
        tokens.add("[CLS]")  // BERT requires CLS token at start
        
        // Simple whitespace + punctuation tokenization (WordPiece approximation)
        val words = text.lowercase().split(Regex("\\s+"))
        for (word in words) {
            val wordTokens = tokenizeWord(word)
            tokens.addAll(wordTokens)
            if (tokens.size >= 254) break  // Leave room for [SEP]
        }
        
        tokens.add("[SEP]")  // BERT requires SEP token at end
        return tokens
    }
    
    private fun tokenizeWord(word: String): List<String> {
        // Check if whole word is in vocab
        if (vocab.containsKey(word)) {
            return listOf(word)
        }
        
        // WordPiece: try subwords with ## prefix
        val subwords = mutableListOf<String>()
        var start = 0
        while (start < word.length) {
            var end = word.length
            var foundSubword: String? = null
            
            while (start < end) {
                val substr = if (start == 0) {
                    word.substring(start, end)
                } else {
                    "##" + word.substring(start, end)
                }
                
                if (vocab.containsKey(substr)) {
                    foundSubword = substr
                    break
                }
                end--
            }
            
            if (foundSubword != null) {
                subwords.add(foundSubword)
                start = end
            } else {
                // Unknown token
                subwords.add("[UNK]")
                break
            }
        }
        return subwords
    }
    
    fun convertTokensToIds(tokens: List<String>): IntArray {
        return tokens.map { token ->
            vocab[token] ?: vocab["[UNK]"] ?: 0
        }.toIntArray()
    }
}
```
</action>
<test>
BertTokenizerWrapper compiles and can load vocab.txt.
</test>
</task>

### Task 3: Create Hilt Dependency Injection Module
<task>
<action>
Create `EmbeddingModule.kt`:

```kotlin
import android.content.Context
import dagger.Module
import dagger.Provides
import dagger.hilt.InstallIn
import dagger.hilt.android.qualifiers.ApplicationContext
import dagger.hilt.components.SingletonComponent
import javax.inject.Singleton

@Module
@InstallIn(SingletonComponent::class)
object EmbeddingModule {
    
    @Provides
    @Singleton
    fun provideEmbeddingModelManager(
        @ApplicationContext context: Context
    ): EmbeddingModelManager {
        return EmbeddingModelManager(context).apply {
            initialize()  // Initialize on app startup
        }
    }
}
```
</action>
<test>
Hilt module compiles and provides EmbeddingModelManager.
</test>
</task>

### Task 4: Add Model Initialization to Application Class
<task>
<action>
Inject EmbeddingModelManager in `NewsThreadApplication.kt` to trigger initialization on app startup:

```kotlin
@HiltAndroidApp
class NewsThreadApplication : Application() {
    
    @Inject
    lateinit var embeddingModelManager: EmbeddingModelManager
    
    override fun onCreate() {
        super.onCreate()
        // EmbeddingModelManager.initialize() called automatically via provider
        Timber.plant(Timber.DebugTree())
    }
}
```
</action>
<test>
App builds. Model initializes on app startup (check Logcat for "TF Lite model initialized").
</test>
</task>

## Verification Criteria

- [ ] EmbeddingModelManager created with model loading logic
- [ ] BertTokenizerWrapper implemented
- [ ] Text truncation at sentence boundary works
- [ ] Dynamic text length based on device RAM implemented
- [ ] Hilt module provides EmbeddingModelManager
- [ ] Model initializes on app startup
- [ ] App builds successfully

## Must-Haves (Goal-Backward Verification)

**Phase 3 Goal**: Generate 384-dim embeddings in <200ms

**This plan must deliver**:
- Working embedding generation engine
- Model loads from assets without corruption
- Inference runs on background thread (Dispatchers.Default)

## Notes

- This plan creates the ENGINE only - no UI, no repository integration yet
- Plan 03-04 will connect this to the repository layer
- Tokenizer is simplified WordPiece (production-ready but not perfect)
- Test manually with sample text to verify embedding generation
